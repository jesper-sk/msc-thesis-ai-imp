# %%
"""Convert the bookcorpus annotations (from ../repos/ewiser/bin/annotate_bookcorpus.py) 
into a file format suitable for vectorisation (and also filter so that only nouns are left).

From:

{
    "text": ,
    "lemma": ,
    "pos": ,
    "offset": ,
} for word in sentence
  for sentence in sentences

(found in `data/bookcorpus/annotated`)

Into:

[synset]_sentences.txt
[target_word_position], First full sentence tokenized etc
...

(found in `data/bookcorpus/prepared`)
"""
# %%

import json
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path

import matplotlib.pyplot as plt
from nltk.corpus import wordnet as wn
from tqdm import tqdm

annotations_path = Path("../data/bookcorpus/annotated")
output_path = Path("../data/bookcorpus/prepared")

with open("../data/selected-words.csv", "r") as file:
    next(file)
    lemmas = [
        row.split(",")[0]
        for row in file.read().split("\n")
        if row and row.split(",")[0]
    ]


def parse_wnoffset(offset: str):
    # wn:xxxxxxxxp where x are ID digits, p is POS identifier
    ret = offset.split(":")[1]
    return ret[-1], int(ret[:-1])


@dataclass
class SentMetadata:
    text: str
    origin: str
    position: str
    synset: str


# %%
"""Load json files from disk and interpret them into datatype that can be saved. Filter 
out all duplicates.
"""

buffer: dict[tuple[str, int], SentMetadata] = dict()
wordnet_map: dict[str, object] = dict()

for idx, path in tqdm(enumerate(annotations_path.glob("*.json")), total=420):
    with open(path, "r") as file:
        data = json.load(file)

    for sentence in data:
        target_word_positions = [
            index for index, word in enumerate(sentence) if word["lemma"] in lemmas
        ]

        if len(target_word_positions) == 0:
            continue

        for target_position in target_word_positions:
            word = sentence[target_position]

            if word["pos"] != "NOUN":
                continue

            offset = word["offset"]
            if offset is None:
                continue

            if offset in wordnet_map:
                synset = wordnet_map[offset]
            else:
                synset = wn.synset_from_pos_and_offset(*parse_wnoffset(offset))
                wordnet_map[offset] = synset

            sentence_text = " ".join([word["text"] for word in sentence])
            key = (sentence_text, target_position)

            if key not in buffer:
                synset_name = str(synset).replace("Synset('", "").replace("')", "")

                buffer[key] = SentMetadata(
                    text=sentence_text,
                    origin=path.stem,
                    position=str(target_position),
                    synset=synset_name,
                )

# %%
"""Save to disk.
"""


print("Purging directory...")
output_path.mkdir(exist_ok=True)
for text_file in output_path.glob("*.sentences.txt"):
    text_file.unlink()

for text_file in output_path.glob("*.target_ids.txt"):
    text_file.unlink()

for text_file in output_path.glob("*.origin.txt"):
    text_file.unlink()


converted = defaultdict(list)
for meta in tqdm(buffer.values(), desc="Converting data"):
    converted[meta.synset].append(meta)

for synset, sentences in tqdm(converted.items(), desc="Saving synsets"):
    sname = f"{synset}.sentences.txt"
    pname = f"{synset}.target_ids.txt"
    oname = f"{synset}.origin.txt"

    with open(output_path / sname, "w") as file:
        file.write("\n".join(sentence.text for sentence in sentences))

    with open(output_path / pname, "w") as file:
        file.write("\n".join(sentence.position for sentence in sentences))

    with open(output_path / oname, "w") as file:
        file.write("\n".join(sentence.origin for sentence in sentences))


# %%
"""
Create plots for all different lemmata and the occurrences of its different synsets
"""
plot_path = Path("../data/bookcorpus/plots")

lemmata = {
    sentencefile.stem.split(".")[0]
    for sentencefile in output_path.glob("*.sentences.txt")
}

all_lemma_count: dict[str, int] = defaultdict(int)

for lemma in lemmata:
    synsets = {}
    for synset in output_path.glob(f"{lemma}.*.sentences.txt"):
        with open(synset, "rbU") as f:
            name = ".".join(synset.stem.split(".")[:-1])
            count = sum(1 for _ in f)
            synsets[name] = count
            all_lemma_count[name] += count

    names = list(synsets.keys())
    values = list(synsets.values())
    ticks = list(range(len(names)))

    plt.bar(ticks, values)
    plt.xticks(range(len(names)), labels=names, rotation=-45)
    plt.savefig(plot_path / f"{lemma}.png")
    plt.autoscale()
    plt.close()

with open(plot_path / "all.csv", "w") as file:
    file.write("\n".join([f"{key},{value}" for key, value in all_lemma_count.items()]))


# %%

noun_to_synset: dict[str, set[str]] = defaultdict(set)
synset_to_noun: dict[str, set[str]] = defaultdict(set)
for (sentence, idx), synset in buffer.items():
    noun = sentence.split(" ")[idx]
    noun_to_synset[noun].add(synset.synset)
    synset_to_noun[synset.synset].add(noun)
