"""This file includes the code that calculates conceptors from the synset state clouds
in data/bookcorpus/prepared/*.embeddings.npy. These can be created using the `vectorise`
command in the CLI (see `pipenv run python app vectorise --help`).
"""

# %%

import itertools as it
from math import sqrt
from pathlib import Path

import numpy as np
import numpy.linalg as la
from tqdm import tqdm

from wsd.conceptors.conceptor import (
    Conceptor,
    loewner,
    posneg_count_fraction,
    posneg_magnitude_fraction,
)
from wsd.data.bookcorpus import Lemma, get_synset_conceptors, get_synset_embeddings

# %%

p = Path("../data/bookcorpus/prepared")

embeddings = get_synset_embeddings(p)
conceptors = get_synset_conceptors(synset_embeddings=embeddings)

# %%
"""Conceptor aperture estimation as per cwsd. Pick an aperture such that the asymptote of 
f(x) = x / (x + a**-2) is around the max singular value of the embedding correlation 

"""
corrs = (Conceptor.make_correlation_matrix(e) for e in embeddings.values())
singular_values = np.array(
    [la.svd(c)[1] for c in tqdm(corrs, total=len(embeddings))]
).flatten()
singular_values.sort()


# %% Helper methods


def ap(max_sv, tol=1e-2):
    return sqrt(1 - tol) / (sqrt(max_sv) * sqrt(tol))


def c(lemma: Lemma, idx: int):
    return conceptors[lemma][idx]


def e(lemma: Lemma, idx: int):
    return embeddings[lemma][idx]


def sent(lemma: Lemma, idx: int, count: int = 1, offset: int = 0):
    with open(
        f"../data/bookcorpus/prepared/{lemma}.n.{idx:02d}.sentences.txt", "r"
    ) as file:
        return list(it.islice(file, offset, offset + count))


___wordnet_definitions: dict[str, str] = {}


def key(lemma: Lemma, idx: int):
    return f"{lemma}.n.{idx:02d}"


def wn(lemma: Lemma, idx: int):
    from nltk.corpus import wordnet

    return wordnet.synset(key(lemma, idx))


def defn(lemma: Lemma, idx: int):
    from nltk.corpus import wordnet

    key = f"{lemma}.n.{idx}"
    return ___wordnet_definitions.get(key, wordnet.synset(key).definition())  # type: ignore


def count(lemma: Lemma):
    return len(embeddings[lemma])


fr = posneg_magnitude_fraction

# %%
plant = c("plant", 2)
fruit = c("fruit", 1)
apple = c("apple", 1)
berry = c("berry", 1)
banana = c("banana", 2)
flower = c("flower", 1)
daffodil = c("daffodil", 1)
blackberry = c("blackberry", 1)
branch = c("branch", 2)
petal = c("petal", 1)
lilac = c("lilac", 1)
# %%
combis = [
    ("fruit", "berry"),
    ("fruit", "banana"),
    ("fruit", "apple"),
    ("apple", "banana"),
    ("berry", "blackberry"),
    ("fruit", "blackberry"),
    ("plant", "flower"),
    ("flower", "daffodil"),
    ("flower", "lilac"),
    ("plant", "daffodil"),
    ("daffodil", "lilac"),
    ("flower", "petal"),
    ("plant", "petal"),
    ("banana", "lilac"),
]

for strf, strs in combis:
    fst = eval(strf)
    snd = eval(strs)
    print(f"{strf} >= {strs}: {fr(fst, snd):.2f}")

# %% Get all the labels and concpetors

all_labels, all_conceptors = zip(
    *[
        (f"{lemma}.{idx}", conceptor)
        for lemma, lemma_conceptors in conceptors.items()
        for idx, conceptor in lemma_conceptors.items()
    ]
)

# %%  Calculate the full SSM
n = len(all_conceptors)
ssm = np.zeros((n, n))
gen = it.combinations(enumerate(all_conceptors), 2)

for (i, c1), (j, c2) in tqdm(gen, total=45451):
    val = fr(c1, c2)
    ssm[i, j] = val
    ssm[j, i] = -val

np.save("../data/bookcorpus/ssm_new_ap0.58.npy", ssm)

import os

os.system("shutdown /s /f")
# %%
with open("../data/bookcorpus/ssm_labels.txt", "r") as file:
    labels = [f.strip().split(".") for f in file.readlines()]

data = [(key(l[0], int(l[1])), wn(l[0], int(l[1]))) for l in labels]

with open("../data/bookcorpus/ssm_labels.csv", "w") as file:
    for this_key, this_wn in data:
        file.write(
            f"{this_key}|{this_wn.lexname()}|{this_wn.definition()}|{(this_wn.examples() or [''])[0]}\n"
        )

# %%


def format_label(label):
    parts = label.strip().split(".")
    return f"{parts[0]}.{int(parts[2])}"


with open("../data/bookcorpus/plots/all.csv", "r") as file:
    count_dict = {
        format_label(x[0]): int(x[1].strip())
        for x in [line.split(",") for line in file.readlines()]
    }

# %%

conc_dict = {lab: conc for lab, conc in zip(all_labels, all_conceptors)}
counts, concs = zip(*[(count_dict[lab], conc_dict[lab]) for lab in all_labels])

sorted_idcs = np.argsort(counts)
sorted_counts = np.take_along_axis(np.asarray(counts), sorted_idcs, None)
traces = [c.quota for c in concs]
sorted_traces = np.take_along_axis(np.asarray(traces), sorted_idcs, None)
# %%
fig, ax = plt.subplots(1, 1, figsize=(6 * 0.9, 4 * 0.9))
fig.tight_layout()

# ax.plot(sorted_counts, c='black', ls=':', label="Occurrence count")
ax.plot(np.arange(768) / 768, ls=":", c="gray")
ax.scatter(sorted_counts, sorted_traces, c="black", s=5)
ax.set_ylim([0, max(sorted_traces) + 0.01])
ax.set_xscale("log")
ax.set_xlabel("Synset occurrence count (log)")
ax.set_ylabel("Conceptor quota")

fig.savefig("../data/plots/trace.pdf", bbox_inches="tight", pad_inches=0)

# %%

fig, ax = plt.subplots(1, 1, figsize=(6 * 0.9, 4 * 0.9))
fig.tight_layout()

ax.scatter(
    range(302), sorted_counts / 768, c="gray", s=1, label="Normalised occurrence count"
)
ax.scatter(range(302), sorted_traces, c="black", s=5, label="Quota")
ax.set_yscale("log")
ax.set_ylabel("Value (log)")
ax.set_xlabel("Conceptor â„–")
ax.legend()

fig.savefig("../data/plots/trace1.pdf", bbox_inches="tight", pad_inches=0)

# %%
from matplotlib import collections as matcoll

fig, ax = plt.subplots(1, 1, figsize=(12, 4))
fig.tight_layout()

x = range(302)
y = list(zip(sorted_counts / 768, sorted_traces))

lines = []
for i, j in zip(x, y):
    pair = [(i, j[0]), (i, j[1])]
    lines.append(pair)

linecoll = matcoll.LineCollection(lines, colors="k")

ax.plot(x, [i for (i, j) in y], "rs", markersize=4)
ax.plot(x, [j for (i, j) in y], "bo", markersize=4)
ax.add_collection(linecoll)
ax.set_yscale("log")
