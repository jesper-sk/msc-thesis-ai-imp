# %%
"""
Convert the bookcorpus annotations (from ../repos/ewiser/bin/annotate_bookcorpus.py) into
a file format suitable for vectorisation (and also filter so that only nouns are left).
See below:
"""

# {
#     "text": ,
#     "lemma": ,
#     "pos": ,
#     "offset": ,
# } for word in sentence
#   for sentence in sentences

# Into:

# [synset]_sentences.txt
# [target_word_position], First full sentence tokenized etc
# ...
# %%

import json
from collections import defaultdict
from pathlib import Path

import matplotlib.pyplot as plt
from nltk.corpus import wordnet as wn

annotations_path = Path("../data/bookcorpus/annotated")
output_path = Path("../data/bookcorpus/prepared")

with open("../data/selected-words.csv", "r") as file:
    next(file)
    lemmas = [
        row.split(",")[0]
        for row in file.read().split("\n")
        if row and row.split(",")[0]
    ]


def parse_wnoffset(offset: str):
    # wn:xxxxxxxxp where x are ID digits, p is POS identifier
    ret = offset.split(":")[1]
    return ret[-1], int(ret[:-1])


# %%
wordnet_map: dict[str, object] = {}
sentence_buffer = defaultdict(list)
position_buffer = defaultdict(list)

print("purging directory...")
output_path.mkdir(exist_ok=True)
for text_file in output_path.glob("*.sentences.txt"):
    text_file.unlink()

for text_file in output_path.glob("*.target_ids.txt"):
    text_file.unlink()

for idx, path in enumerate(annotations_path.glob("*.json")):
    print(f"{idx:03d}:", path.stem)
    with open(path, "r") as file:
        data = json.load(file)

    for sentence in data:
        target_word_positions = [
            index for index, word in enumerate(sentence) if word["lemma"] in lemmas
        ]

        if len(target_word_positions) == 0:
            continue

        for target_position in target_word_positions:
            word = sentence[target_position]

            if word["pos"] != "NOUN":
                continue

            offset = word["offset"]
            if offset is None:
                continue

            if offset in wordnet_map:
                synset = wordnet_map[offset]
            else:
                synset = wn.synset_from_pos_and_offset(*parse_wnoffset(offset))
                wordnet_map[offset] = synset

            synset_name = str(synset).replace("Synset('", "").replace("')", "")
            sname = f"{synset_name}.sentences.txt"
            pname = f"{synset_name}.target_ids.txt"

            sentence_buffer[sname].append(" ".join([word["text"] for word in sentence]))
            position_buffer[pname].append(str(target_position))

    if (idx + 1) % 5 == 0:
        print("saving")
        for filename, sentences in sentence_buffer.items():
            with open(output_path / filename, "a") as file:
                file.write("\n".join(sentences))

        for filename, positions in position_buffer.items():
            with open(output_path / filename, "a") as file:
                file.write("\n".join(positions))

        sentence_buffer = defaultdict(list)
        position_buffer = defaultdict(list)


# %%
"""
Create plots for all different lemmata and the occurrences of its different synsets
"""
plot_path = Path("../data/bookcorpus/plots")

lemmata = {
    sentencefile.stem.split(".")[0]
    for sentencefile in output_path.glob("*.sentences.txt")
}

all_lemma_count: dict[str, int] = defaultdict(int)

for lemma in lemmata:
    synsets = {}
    for synset in output_path.glob(f"{lemma}.*.sentences.txt"):
        with open(synset, "rbU") as f:
            name = ".".join(synset.stem.split(".")[:-1])
            count = sum(1 for _ in f)
            synsets[name] = count
            all_lemma_count[name] += count

    names = list(synsets.keys())
    values = list(synsets.values())
    ticks = list(range(len(names)))

    plt.bar(ticks, values)
    plt.xticks(range(len(names)), labels=names, rotation=-45)
    plt.savefig(plot_path / f"{lemma}.png")
    plt.autoscale()
    plt.close()

with open(plot_path / "all.csv", "w") as file:
    file.write("\n".join([f"{key},{value}" for key, value in all_lemma_count.items()]))
