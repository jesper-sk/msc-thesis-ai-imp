# %%
# {
#     "text": ,
#     "lemma": ,
#     "pos": ,
#     "offset": ,
# } for word in sentence
#   for sentence in sentences

# Into:

# [synset]_sentences.txt
# [target_word_position], First full sentence tokenized etc
# ...
# %%

import json
from pathlib import Path

from nltk.corpus import wordnet as wn
from tqdm import tqdm

annotations_path = Path("../data/bookcorpus/annotated")
output_path = Path("../data/bookcorpus/prepared")

with open("../data/selected-words.csv", "r") as file:
    next(file)
    lemmas = [
        row.split(",")[0]
        for row in file.read().split("\n")
        if row and row.split(",")[0]
    ]


def parse_wnoffset(offset: str):
    # wn:xxxxxxxxp where x are ID digits, p is POS identifier
    ret = offset.split(":")[1]
    return ret[-1], int(ret[:-1])


# %%
wordnet_map = {}

for path in annotations_path.glob("*.json"):
    with open(path, "r") as file:
        data = json.load(file)

    for sentence in tqdm(data):
        target_word_positions = [
            index for index, word in enumerate(sentence) if word["lemma"] in lemmas
        ]

        if len(target_word_positions) == 0:
            continue

        for target_position in target_word_positions:
            word = sentence[target_position]

            if word["pos"] != "NOUN":
                continue

            offset = word["offset"]
            if offset is None:
                continue

            if offset in wordnet_map:
                synset = wordnet_map[offset]
            else:
                synset = wn.synset_from_pos_and_offset(*parse_wnoffset(offset))
                wordnet_map[offset] = synset

            spath = output_path / f"{synset}.sentences.txt"
            ipath = output_path / f"{synset}.target_ids.txt"

            with open(spath, "a") as file:
                file.write(" ".join([word["text"] for word in sentence]))
                file.write("\n")

            with open(ipath, "a") as file:
                file.write(f"{target_position}\n")

        break


# %%
